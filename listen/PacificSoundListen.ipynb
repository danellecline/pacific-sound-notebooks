{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ![alt text](https://www.mbari.org/wp-content/uploads/2014/11/logo-mbari-3b.png \"MBARI\")\n",
    "\n",
    "  <div align=\"left\">Copyright (c) 2022, MBARI</div>\n",
    "\n",
    "  * Distributed under the terms of the GPL License\n",
    "  * Maintainer: John Ryan ryjo@mbari.org\n",
    "  * Authors: John Ryan ryjo@mbari.org, Carlos Rueda carueda@mbari.org, Danelle Cline dcline@mbari.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listening to *Pacific Ocean Sound* recordings\n",
    "\n",
    "Ocean sound recordings have tremendous potential for use by sound artists, naturalists, and educators whose primary goal is to listen rather than analyze.  This notebook illustrates how to extract a period of recording from the *Pacific Ocean Sound* archive and process it for listening.  After processing, you can listen right in this notebook, and you can download the processed file for use elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a time to listen\n",
    "\n",
    "Approaching 7 years of continuous recording, this growing audio archive offers an unfathomable depth of potential listening.  How can you survey this sea of sound for a period that may be of interest for listening?\n",
    "\n",
    "One resource is knowledge of the behavioral ecology of species that you want to hear.  For example, the songs of baleen whales (especially humpback, blue and fin whales) can be heard largely during fall where we are recording (off central California).  So, listening during any time between about August and November, you are likely to encounter their songs.\n",
    "\n",
    "A more focused way to search is to use our [visual browsing interface](https://www.mbari.org/at-sea/cabled-observatory/mars-science-experiments/mars-hydrophone-data/) (see *Browsing the spectrograms*).  Here, the soundscape comes to life visually, and the visual signatures of different species can be viewed hour by hour for any period of recording since the start of recording in July 2015.  Once you become familiar with how the sounds of different species appear visually in a spectrogram, you can easily select a time when an interesting sound pattern is visually evident in the soundscape, and you can use the methods presented here to listen.  The only information you need from the visual browsing interface is the date and time range of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example sounds\n",
    "\n",
    "Below we'll list times when you can extract and listen to the sounds of different species.  The vocalizations of some species are abundant, however hearing them requires a subwoofer speaker or good headphones because the sound has such a low pitch.  So, for these examples we'll focus on vocalizations of species that can be easily heard through ordinary computer speakers: *humpback whales, orcas, and dolphins*.\n",
    "\n",
    "The original recordings have a sample rate of 256 kHz, while the upper limit of human hearing is about 20 kHz.  So the original recordings are a bit much for just listening.  For demonstration purposes here, we'll tap into a version of the  audio data that covers part of our hearing range: 16 kHz data (representing sounds up to 8 kHz).  A companion notebook will illustrate how to listen to higher frequency sounds, including slowing down playback of sounds that are above the range of human hearing so that they can be heard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required dependencies\n",
    "\n",
    "First, let's install the required software dependencies. If you are working on local computer, you can skip this next cell. Change your kernel to *pacific-sound-notebooks*, which you installed according to the instructions in the [README](https://github.com/mbari-org/pacific-sound-notebooks/) - this has all the dependencies that are needed. \n",
    "\n",
    "Otherwise, if you are using this notebook in a cloud jupyter notebook, select a Python3 compatible kernel, remove the comment # before each line and run the code cell.  This only needs to be done once for the duration of this notebook.\n",
    "\n",
    "***This step will require about 30 seconds to complete.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get update -y && apt-get install -y libsndfile1\n",
    "# !apt-get install libsox-fmt-all libsox-dev sox > /dev/null\n",
    "# !pip install sox --quiet\n",
    "# !pip install panel --quiet\n",
    "# !pip install soundfile --quiet\n",
    "# !pip install holoviews --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import sox\n",
    "from six.moves.urllib.request import urlopen\n",
    "import io\n",
    "import soundfile as sf\n",
    "# Add widget for playback and spectrogram with playhead:\n",
    "from scipy.signal import spectrogram\n",
    "import panel as pn\n",
    "pn.extension()\n",
    "import holoviews as hv\n",
    "hv.extension(\"bokeh\", logo=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract a recording from the archive\n",
    "\n",
    "Now, let's pull examples from the data archive.  The code below allows you to specify one of a few species (change the name inside the quotes for the line at the top beginning with *animal =* .  For each example the start time and duration of a recording segment are already defined.\n",
    "\n",
    "***This step will require about 150 seconds to complete.*** (The slowness is due to reading an entire daily file before subsetting; this may be sped up in future versions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing this processing, you can specify from a number of examples:\n",
    "# humpback whale, orcas, dolphins\n",
    "animal = \"orcas\"  # modify the selection inside the quotes\n",
    "\n",
    "if animal == \"humpback whale\":\n",
    "  # year, month, day, starthour, startminute, startsecond = 2017, 11, 7, 23, 24, 23\n",
    "  year, month, day, starthour, startminute, startsecond = 2015, 12, 7, 6, 58, 15\n",
    "elif animal == \"orcas\":\n",
    "  year, month, day, starthour, startminute, startsecond = 2018, 4, 13, 8, 42, 35\n",
    "elif animal == \"dolphins\":\n",
    "  year, month, day, starthour, startminute, startsecond = 2015, 7, 31, 23, 12, 11\n",
    "\n",
    "readseconds = 55  # for all examples; you can modify for longer segments\n",
    "\n",
    "print(f'Extracting recording of {animal}: {readseconds} seconds starting {year}/{month:0>2}/{day:0>2} {starthour:0>2}:{startminute:0>2}:{startsecond:0>2}')\n",
    "filename = f'MARS-{year}{month:0>2}{day:0>2}T000000Z-16kHz.wav'\n",
    "url = f'https://pacific-sound-16khz.s3.amazonaws.com/{year}/{month:0>2}/{filename}'\n",
    "print(f'Loading audio data from {url}')\n",
    "\n",
    "# Might load data from archives having different sample rates, so read that first.\n",
    "_, sample_rate = sf.read(io.BytesIO(urlopen(url).read(1000)), dtype='float32')\n",
    "print(f'File audio sample rate   = {sample_rate} Hz')\n",
    "\n",
    "startsampl = starthour*60*60*sample_rate + startminute*60*sample_rate + startsecond*sample_rate;\n",
    "endsampl = startsampl + readseconds*sample_rate\n",
    "\n",
    "psound_segment, _ = sf.read(io.BytesIO(urlopen(url).read()), start=startsampl, stop=endsampl, dtype='float32')\n",
    "nsec = psound_segment.size/sample_rate\n",
    "print(f'Loaded {nsec} seconds')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the raw audio data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see why we need to modify the raw audio data for listening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a time reference for the raw audio segment\n",
    "segment_seconds = numpy.linspace(0,nsec,len(psound_segment))\n",
    "\n",
    "plt.figure(dpi=200, figsize = [7,3])\n",
    "plt.plot(segment_seconds,psound_segment)\n",
    "plt.xlabel('Seconds')\n",
    "plt.ylabel('Signal (raw, scaled voltage)')\n",
    "plt.title('Waveform of original audio data')\n",
    "plt.autoscale(enable=True, axis='x', tight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify for listening\n",
    "Attributes of the raw audio (above) that we need to change for listening are:\n",
    "* remove the DC offset (center on zero)\n",
    "* increase the signal amplitude\n",
    "\n",
    "For this, we'll use Sound eXchange (SoX)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SoX transformer with:\n",
    "# * a highpass filter to remove the DC offset\n",
    "# * fade in/out for easy listening\n",
    "# * normalization to increase the amplitude\n",
    "tfm = sox.Transformer()\n",
    "tfm.highpass(10)\n",
    "tfm.fade(fade_in_len=1, fade_out_len=1)\n",
    "tfm.norm(db_level = -3.0)\n",
    "processed_audio = tfm.build_array(input_array=psound_segment, sample_rate_in=sample_rate)\n",
    "\n",
    "# View the processed audio segment\n",
    "plt.figure(dpi=200, figsize = [7,3])\n",
    "plt.plot(segment_seconds,processed_audio)\n",
    "plt.xlabel('Seconds')\n",
    "plt.ylabel('Signal (scaled for listening)')\n",
    "plt.title('Waveform of processed audio data')\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "\n",
    "# Save the result as a wav file\n",
    "OUTPUT_FILENAME = 'audio_data.wav'\n",
    "sf.write(OUTPUT_FILENAME, processed_audio, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The processed audio data plotted above has been saved to the file audio_data.wav.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listen\n",
    "In this section, you can listen to your processed audio data in two ways.  Try both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sound only\n",
    "After you run the code, the player will appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playback:\n",
    "import IPython\n",
    "IPython.display.Audio(OUTPUT_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sound and its visual representation (spectrogram)\n",
    "After you run the code, the player will appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = segment_seconds\n",
    "\n",
    "f, t, sxx = spectrogram(processed_audio, sample_rate, window='hann')\n",
    "\n",
    "spec_gram = hv.Image((t, f, numpy.log10(sxx)), \n",
    "                     [\"Time (s)\", \"Frequency (Hz)\"]\n",
    "                     ).opts(width=1000)\n",
    "\n",
    "spec_gram_c = spec_gram.opts(cmap='gray_r')\n",
    "\n",
    "audio = pn.pane.Audio(OUTPUT_FILENAME, sample_rate=sample_rate, name='Audio', throttle=50)\n",
    "\n",
    "def update_playhead(x,y,t):\n",
    "    if x is None:\n",
    "        return hv.VLine(t + 0.5)\n",
    "    else:\n",
    "        audio.time = x\n",
    "        return hv.VLine(x + 0.5)\n",
    "\n",
    "tap_stream = hv.streams.SingleTap(transient=True)\n",
    "time_play_stream = hv.streams.Params(parameters=[audio.param.time], rename={'time': 't'})\n",
    "dmap_time = hv.DynamicMap(update_playhead, streams=[time_play_stream, tap_stream])\n",
    "\n",
    "pn.Column(spec_gram_c * dmap_time, audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "* The 16 kHz data are great for listening, but they do not fully cover the range of human hearing.  When you view the spectrogram of the example dolphin recording, you may notice that the sound energy extends up to the limit of this decimated recording (8 kHz).  Sounds from the dolphins certainly extend above this, and a companion notebook will illustrate how to extract audio spanning the full range of human hearing.\n",
    "* A convenient way to transition between the example sounds is to (1) [clear all outputs], and (2) [restart and run all]."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SoundscapeListen.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pacific-sound-notebooks",
   "language": "python",
   "name": "pacific-sound-notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}