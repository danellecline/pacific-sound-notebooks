{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ![alt text](https://www.mbari.org/wp-content/uploads/2014/11/logo-mbari-3b.png \"MBARI\")\n",
    "\n",
    "  <div align=\"left\">Copyright (c) 2022, MBARI</div>\n",
    "\n",
    "  * Distributed under the terms of the GPL License\n",
    "  * Maintainer: carueda@mbari.org\n",
    "  * Authors: Carlos Rueda carueda@mbari.org, John Ryan ryjo@mbari.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the Google Humpback Whale Model on Pacific Ocean Sound data\n",
    "\n",
    "This notebook exercises the *Humpback whales acoustic detector (by NOAA & Google)* [1] on *Pacific Ocean Sound* [2] data.  Although the model was developed using recordings from breeding habitat of the central and western North Pacific, it proves to be effective when applied to recordings from foraging habitat of the California Current System.\n",
    "\n",
    "For a 30-minute segment of audio in a given year-month-day,\n",
    "we apply the model and plot a spectrogram of the corresponding signal along with the song presence scores obtained by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required dependencies\n",
    "\n",
    "First, let's install the required software dependencies. \n",
    "\n",
    "If you are using this notebook in a cloud environment, select a Python3 compatible kernel and run this next section.  This only needs to be done once for the duration of this notebook.\n",
    "\n",
    "If you are working on local computer, you can skip this next cell. Change your kernel to *pacific-sound-notebooks*, which you installed according to the instructions in the [README](https://github.com/mbari-org/pacific-sound-notebooks/) - this has all the dependencies that are needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update -y && apt-get install -y libsndfile1\n",
    "!pip install boto3 --quiet\n",
    "!pip install tensorflow==2.4.1 --quiet\n",
    "!pip install soundfile --quiet\n",
    "!pip install tensorflow_hub --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import librosa\n",
    "from matplotlib import gridspec\n",
    "import scipy\n",
    "from six.moves.urllib.request import urlopen\n",
    "import io\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supporting code - plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram_scipy(signal, sample_rate, \n",
    "                           hydrophone_sensitivity,\n",
    "                           title=None,\n",
    "                           with_colorbar=True\n",
    "                           ):\n",
    "\n",
    "    # Compute spectrogram:    \n",
    "    w = scipy.signal.get_window('hann', sample_rate)\n",
    "    f, t, psd = scipy.signal.spectrogram(signal, \n",
    "                                         sample_rate, \n",
    "                                         nperseg=sample_rate, \n",
    "                                         noverlap=0,\n",
    "                                         window=w,\n",
    "                                         nfft=sample_rate,\n",
    "                                         )\n",
    "    psd = 10*np.log10(psd) - hydrophone_sensitivity\n",
    "\n",
    "    # Plot spectrogram:\n",
    "    plt.imshow(psd, aspect='auto', origin='lower',\n",
    "               vmin=30, vmax=90,\n",
    "               cmap='Blues',\n",
    "               )\n",
    "    plt.yscale('log')\n",
    "    y_max = sample_rate / 2\n",
    "    plt.ylim(10, y_max)\n",
    "\n",
    "    if with_colorbar:\n",
    "        plt.colorbar()\n",
    "\n",
    "    plt.xlabel('Seconds')\n",
    "    plt.ylabel('Frequency (Hz)')\n",
    "    plt.title(title or \\\n",
    "              f'Calibrated spectrum levels, 16 {sample_rate / 1000.0} kHz data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(scores, \n",
    "                with_steps=False,\n",
    "                with_dots=True,\n",
    "                med_filt_size=None,\n",
    "                ):\n",
    "\n",
    "    if with_steps:\n",
    "        # repeat last value to also see a step at the end:\n",
    "        scores = np.concatenate((scores, scores[-1:]))\n",
    "        x = range(len(scores))\n",
    "        plt.step(x, scores, where='post')\n",
    "    else:\n",
    "        x = range(len(scores))\n",
    "\n",
    "    if with_dots:\n",
    "        plt.plot(x, scores, 'o', color='lightgrey', markersize=9)\n",
    "\n",
    "    plt.grid(axis='x', color='0.95')\n",
    "    plt.xlim(xmin=0, xmax=len(scores) - 1)\n",
    "    plt.ylabel('Model Score')\n",
    "    plt.xlabel('Seconds')\n",
    "\n",
    "    if med_filt_size is not None:\n",
    "        scores_int = [int(s*1000) for s in scores]\n",
    "        meds_int = scipy.signal.medfilt(scores_int, kernel_size=med_filt_size)\n",
    "        meds = [m/1000. for m in meds_int]\n",
    "        plt.plot(x, meds, 'p', color='black', markersize=9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined spectrogram and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(scores,\n",
    "                 context_step_samples,\n",
    "                 signal,\n",
    "                 sample_rate,\n",
    "                 hydrophone_sensitivity,\n",
    "                 title,\n",
    "                 scores_with_dots=True,\n",
    "                 scores_with_steps=False,\n",
    "                 scores_med_filt_size=None,\n",
    "                 ):\n",
    "\n",
    "    signal_len_per_scores = len(scores) * context_step_samples \n",
    "\n",
    "    # Note, the nominal signal length per the obtained score array from the\n",
    "    # model may be larger than the signal given as input.\n",
    "    # When this happens, we discard as many trailing scores as necesary:\n",
    "    while signal_len_per_scores > len(signal):\n",
    "        scores = scores[:-1]\n",
    "        signal_len_per_scores = len(scores) * context_step_samples\n",
    "        # print(f':: adjusting: signal_len_per_scores = {signal_len_per_scores}')\n",
    "\n",
    "    # A final adjustment to make scores and signal \"aligned:\"\n",
    "    signal = signal[:signal_len_per_scores]\n",
    "\n",
    "    # As numpy array:\n",
    "    signal = np.array(signal)\n",
    "\n",
    "    fig = plt.figure(figsize=(24, 8))\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[1, 1])\n",
    "\n",
    "    # Plot spectrogram:\n",
    "    plt.subplot(gs[0])\n",
    "    plot_spectrogram_scipy(signal, sample_rate, hydrophone_sensitivity,\n",
    "                           title,\n",
    "                           with_colorbar=False,\n",
    "                           )\n",
    "\n",
    "    # Plot scores:\n",
    "    fig.add_subplot(gs[1])\n",
    "    plot_scores(scores, \n",
    "                with_dots=scores_with_dots,\n",
    "                with_steps=scores_with_steps,\n",
    "                med_filt_size=scores_med_filt_size,\n",
    "                )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and get its score_fn for use in the exercise:\n",
    "MODEL_URL = 'https://tfhub.dev/google/humpback_whale/1'\n",
    "print(f'== Loading model {MODEL_URL} ...')\n",
    "model = hub.load(MODEL_URL)\n",
    "print('model:', model)\n",
    "\n",
    "score_fn = model.signatures['score']\n",
    "\n",
    "metadata_fn = model.signatures['metadata']\n",
    "metadata = metadata_fn()\n",
    "print('metadata:')\n",
    "for k, v in metadata.items():\n",
    "    print(f'  {k}: {v}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: A 30-minute segment with a 1-sec score resolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will get audio from the 16kHz Pacific Sound data\n",
    "sample_rate = 16_000\n",
    "\n",
    "# The particular segment will be from the file corresponding to this day:\n",
    "year, month, day = 2016, 12, 21\n",
    "\n",
    "# starting at 00h:25m:\n",
    "at_hour, at_minute = 0, 25 \n",
    "\n",
    "# and with a 30-min duration:\n",
    "hours, minutes = 0, 30\n",
    "\n",
    "# The url to download for that day is:\n",
    "filename = f'MARS-{year}{month}{day}T000000Z-16kHz.wav'\n",
    "url = f'https://pacific-sound-16khz.s3.amazonaws.com/{year}/{month}/{filename}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: instead of downloading the whole day file, in this notebook we will\n",
    "# only download enough data to cover the desired 30-min segment indicated above\n",
    "# (including some space for the header of the file).\n",
    "# The number of bytes to read is captured in max_file_size_dl as follows:\n",
    "tot_audio_minutes = (at_hour + hours) * 60 + at_minute + minutes\n",
    "tot_audio_seconds = 60 * tot_audio_minutes\n",
    "tot_audio_samples = sample_rate * tot_audio_seconds\n",
    "tot_audio_bytes = 3 * tot_audio_samples    # 3 because audio is 24-bit\n",
    "max_file_size_dl = 1024 + tot_audio_bytes  # 1024 enough to cover file header\n",
    "\n",
    "# Let's now load the audio:\n",
    "print(f'\\n==> Loading segment from {year}-{month}-{day} @ {at_hour}h:{at_minute}m with duration {hours}h:{minutes}m')\n",
    "psound, _ = sf.read(io.BytesIO(urlopen(url).read(max_file_size_dl)), dtype='float32')\n",
    "# (sf.read also returns the sample rate but we already know it is 16_000)\n",
    "\n",
    "# Get psound_segment from psound based on offset determined by at_hour:at_minute:\n",
    "offset_seconds = (at_hour * 60 + at_minute) * 60\n",
    "offset_samples = sample_rate * offset_seconds\n",
    "psound_segment = psound[offset_samples:]\n",
    "\n",
    "print(f'len(psound)         = {len(psound)}')\n",
    "print(f'len(psound_segment) = {len(psound_segment)}')\n",
    "\n",
    "# The size of psound_segment in seconds as desired is:\n",
    "psound_segment_seconds = (hours * 60 + minutes) * 60\n",
    "\n",
    "print(f'psound_segment_seconds = {psound_segment_seconds}')\n",
    "\n",
    "# So, we have our desired segment from the 16 kHz file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, resample our 16 kHz segment as the model expects 10 kHz audio:\n",
    "print(f'==> Resampling to 10 kHz...')\n",
    "psound_segment_at_10k = librosa.resample(\n",
    "            psound_segment, \n",
    "            orig_sr=sample_rate,\n",
    "            target_sr=10_000,\n",
    "            scale=True  # \"Scale the resampled signal so that y and y_hat have approximately equal total energy\"\n",
    "            )\n",
    "\n",
    "# At 10 kHz, this is the nominal size in samples of our desired segment\n",
    "# when resampled:\n",
    "psound_segment_samples_at_10k = 10_000 * psound_segment_seconds\n",
    "print(f'psound_segment_samples_at_10k = {psound_segment_samples_at_10k}')\n",
    "\n",
    "# Note that psound_segment_at_10k could be a bit larger.\n",
    "# Do the adjustment so it is our desired segment duration:\n",
    "psound_segment_at_10k = psound_segment_at_10k[:psound_segment_samples_at_10k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we apply the model on our segment at 10 kHz:\n",
    "\n",
    "# We specify a 1-sec score resolution:\n",
    "context_step_samples = tf.cast(10_000, tf.int64)\n",
    "\n",
    "print(f'\\n==> Applying model ...')\n",
    "print(f'   len(psound_segment_at_10k) = {len(psound_segment_at_10k)}')\n",
    "\n",
    "waveform1 = np.expand_dims(psound_segment_at_10k, axis=1)\n",
    "waveform_exp = tf.expand_dims(waveform1, 0)  # makes a batch of size 1\n",
    "\n",
    "psound_scores = score_fn(waveform=waveform_exp, \n",
    "                         context_step_samples=context_step_samples\n",
    "                        )\n",
    "score_values = psound_scores['scores'].numpy()[0]\n",
    "print(f'==> Model applied.  Obtained score_values = {len(score_values)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the results, with a plot of the spectrogram along with\n",
    "# the 1-sec resolution score and a median filtered score:\n",
    "\n",
    "print(f'\\n==> Plotting results')\n",
    "\n",
    "title = f'Scores for segment {year}-{month}-{day}'\n",
    "title += f' @ {at_hour}h:{at_minute}m with duration {hours}h:{minutes}m'\n",
    "title += ' (resampled to 10 kHz)'\n",
    "\n",
    "plot_results(score_values,\n",
    "             context_step_samples,\n",
    "             signal=psound_segment_at_10k,\n",
    "             sample_rate=10_000,\n",
    "             hydrophone_sensitivity=-168.8,\n",
    "             title=title,\n",
    "             scores_with_dots=True,\n",
    "             scores_med_filt_size=25,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [1] https://tfhub.dev/google/humpback_whale/1\n",
    "- [2] https://doi.org/10.1109/OCEANS.2016.7761363"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "WSghNg1qXW1G",
    "JW6vqLQrewoM",
    "Occ2plwFYDXW"
   ],
   "name": "Google-Humpback-Whale-Model-on-PacificSound.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "pacific-sound-notebooks",
   "language": "python",
   "name": "pacific-sound-notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}